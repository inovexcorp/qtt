# SPARQi AI Assistant Configuration
# OpenAI-compatible LLM endpoint configuration

# LLM Gateway Configuration
# For OpenAI: https://api.openai.com/v1
# For LiteLLM: http://localhost:4000 (or your LiteLLM gateway URL)
# For Azure OpenAI: https://<your-resource>.openai.azure.com/openai/deployments/<deployment-id>
llmBaseUrl=$[env:SPARQI_LLM_BASE_URL]

# API Key for authentication with LLM provider
# Required for most providers (OpenAI, Anthropic via LiteLLM, etc.)
llmApiKey=$[env:SPARQI_LLM_API_KEY]

# Model name (examples below)
# OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
# LiteLLM proxied models: claude-3-5-sonnet-20241022, gemini-pro, mistral-large
# Azure: your-deployment-name
# Ollama (via LiteLLM): llama3.1, codellama, mistral
llmModelName=$[env:SPARQI_LLM_MODEL]

# Request timeout in seconds
# Increase for longer conversations or slower models
llmTimeout=$[env:SPARQI_LLM_TIMEOUT;default=90]

# Model temperature (0.0-1.0)
# Lower values (0.1-0.3) make output more focused and deterministic
# Higher values (0.7-1.0) make output more creative and varied
llmTemperature=$[env:SPARQI_LLM_TEMPERATURE;default=0.7]

# Maximum tokens per response
# Controls the length of SPARQi's responses
llmMaxTokens=$[env:SPARQI_LLM_MAX_TOKENS;default=4000]

# Session Configuration

# Session timeout in minutes
# Sessions inactive for this duration will be automatically cleaned up
sessionTimeoutMinutes=$[env:SPARQI_SESSION_TIMEOUT;default=60]

# Maximum conversation history messages
# Limits how many messages are kept in memory per session
# Older messages are dropped to manage memory usage
maxConversationHistory=$[env:SPARQI_MAX_CONVO_HISTORY;default=50]

# Enable/disable SPARQi service
# Set to false to disable SPARQi without removing the bundle
enableSparqi=$[env:SPARQI_ENABLED;default=false]

# Welcome Message Template
# Available variables: {{routeId}}, {{ontologyElementCount}}
# This message is shown when a \new SPARQi session is started
welcomeMessageTemplate=Hi! I'm SPARQi, your friendly SPARQL assistant. I can see you're working on the '{{routeId}}' route. I have access to {{ontologyElementCount}} ontology elements from your graphmart. How can I help you with your SPARQL template today?

# System Prompt Template
# Available variables: {{routeId}}, {{routeDescription}}, {{graphMartUri}},
#                      {{layerUris}}, {{ontologyElementCount}}, {{currentTemplateSection}}
# This prompt defines SPARQi's behavior and provides context for each conversation
# Note: Use double-curly syntax for template parameters (e.g., {{paramName}})
systemPromptTemplate=You are SPARQi, a friendly and helpful SPARQL expert who loves to teach. \
You help users develop SPARQL CONSTRUCT queries that use double-curly template parameters (e.g., {{paramName}}).\n\
\n\
Current Route Context:\n\
- Route ID: {{routeId}}\n\
- Description: {{routeDescription}}\n\
- GraphMart URI: {{graphMartUri}}\n\
- Layers: {{layerUris}}\n\
- Ontology Elements Available: {{ontologyElementCount}}\n\
\n\
{{currentTemplateSection}}\
Available Tools:\n\
You have access to ontology lookup tools that let you:\n\
- Search for ontology elements by keywords (lookupOntologyElements)\n\
- Get all classes in the ontology (getAllClasses)\n\
- Get all properties with their domains and ranges (getAllProperties)\n\
- Get individuals/instances to understand data patterns (getIndividuals)\n\
- Get detailed property information (getPropertyDetails)\n\
Use these tools whenever you need to understand what ontology elements are available or find the right classes/properties for a SPARQL query.\n\
\n\
Guidelines:\n\
- Provide clear, concise SPARQL guidance\n\
- Use double-curly syntax for parameters ({{paramName}})\n\
- Focus on CONSTRUCT queries that return JSON-LD\n\
- When users ask about available classes or properties, USE THE TOOLS to get accurate information\n\
- When helping build queries, search the ontology to find relevant concepts\n\
- Reference specific ontology URIs, labels, and relationships from tool results\n\
- Be encouraging and supportive

# Metrics Configuration

# Enable/disable metrics collection
# Set to false to disable tracking of token usage, costs, and usage statistics
metricsEnabled=$[env:SPARQI_METRICS_ENABLED;default=true]

# Cost Estimation Configuration
# These values are used to estimate the cost of LLM API calls based on token usage
# Adjust these to match your actual LLM provider's pricing

# Cost per 1 million input tokens (in USD)
# Default: $2.50 (typical for GPT-4o-mini input tokens)
# Examples: GPT-4o=$2.50, GPT-4-turbo=$10.00, Claude-3.5-Sonnet=$3.00
inputTokenCostPer1M=$[env:SPARQI_INPUT_TOKEN_COST;default=2.50]

# Cost per 1 million output tokens (in USD)
# Default: $10.00 (typical for GPT-4o-mini output tokens)
# Examples: GPT-4o=$10.00, GPT-4-turbo=$30.00, Claude-3.5-Sonnet=$15.00
outputTokenCostPer1M=$[env:SPARQI_OUTPUT_TOKEN_COST;default=10.00]
